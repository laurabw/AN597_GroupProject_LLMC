---
title: "Discriminant Analysis"
author: "Laura Angley, Laura Brubaker-Wittman, Christian Gagnon, Mel Zarate"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    math: mathjax

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Preliminaries

For this module, you will need to install the following packages:

* {curl}
* {MASS}
* {candisc}
* {ggplot2}
* {klaR}
* {mvnormtest}

# Objectives

The objective of this module is to introduce **discriminant analysis** in general as well as describe and go through examples using three different types of disriminant analysis: **linear discriminant analysis (LDA)**, **quadratic discriminant analysis (QDA)**, and **canonical discriminant analysis (CDA)**.

# Introduction

### What is Discriminant Analysis?
**Discriminant analysis** is used when the objective of the assessment of data is to determine the adequacy of classification of data into groups, including a priori groups.This is different than a similar statistical tool, **principal component analysis (PCA)**, which focuses on overall variation in data without regard for a specific grouping of the observations.

In this way, **discriminant analysis** allows us to build classifiers, that is through this type of analysis, we can build a predictive model of group membership that we can then apply to new cases, to see into which group a new case would fall. As opposed to regression techniques, **discriminant analysis** produces group labels, and not a real value as its output.

Furthermore, **discriminant analysis** can be linear or it can fit other curves; it can also be two- or multi-dimensional with either a line or plane separting the categories, depending on the numnber of dimensions. We will see some of the examples of how this works and the kinds of visuals that can be produced as part of this tutorial.

While **discriminant analysis** parallels multiple regression analysis in some ways, one important difference is that regression analysis works with a continuous dependent variable, while **discriminant analysis** must use a discrete dependent variable.

A simple example of **discriminant analysis** would be to consider the following situation: An academic advisor splits a cohort of students into three groups: *those who graduate and do not plan to go on to graduate school*, *those who graduate and go on to graduate school*, and those who *drop out of college*.  A variety of data could be collected for each group during their time in college, before knowing which group they will fall into at the end. After graduation for that cohort, all the students will fall into one of these categories. **Discriminant analysis** could then be used to determine which variable(s) that were collected work as the best predictors of the students' educational outcomes.

## Linear Discriminant Analysis (LDA)

Just like a **PCA** or other **discriminant analysis**, *the goal is to see if certain qualities can describe a categorical variable*. If so, when we plot these analyses, we should be able to see somewhat clear separation of the categories. **LDA** uses data to divide predictor variables into categorical regions with linear boundaries. Compared to **PCA**, **LDA** orders dimensions based on how much separation each category achieves, maximizing the difference and minimizing the overlap of clusters. 

A **PCA** can be thought of as an “unsupervised” algorithm that ignores class labels and searches for the directions (principal components) that maximize variance. **LDA**, however, can be thought of as “supervised,” computing the directions (linear discriminants) that represent the axes in order to maximize separation between different classes. **LDA** tends to be better to use when there are multiple classes and the number of samples per class is relatively small. Also, in comparison with **QDA**, it is assumed that covariances of independent variables is the same for each class. 

<img src = "https://sebastianraschka.com/images/blog/2014/linear-discriminant-analysis/lda_1.png">

Here we have the blue AND green cluster on the x-axis, so if points in the future behave accordingly to the probability density function, then they should be classified into a single cluster.

The LDA returns and output giving scores that define the weight to how much the variables compose the function. These scores are calculated with this equation: 


\[ \delta_i (X) = - \frac{1}{2} \mu_i^T \Sigma^{-1}  X + ln(\pi_i)\]

* **δ** is the discriminant score for class **i**
* **X** is the matrix of independent variables
* **μ** is a vector containing the means of each variable for class **i**
* **Σ** is the covariance matrix of the variables (assumed to be the same for all classes)
* **π** is the prior probability that an observation belongs to class **i**

But why would we ever do this by hand when we have the lovely lda() function in the {MASS} ***R*** package?!

<img src = "https://www.lostandfoundnature.com/wp-content/uploads/Blog/Spreadwing_damselfly/Lagothrix-flavicauda-yellow-tailed-woolly-monkey-Andrew-WalmsleyNPC-3-1024x680.jpg">

##Challenge##

Packages we need: 
```{r}
library(curl)
library(MASS)
library(ggplot2)
```

We can use the Kamilar and Cooper data set to take a look at how this works. We're going to use the Kamala and Cooper dataset, but, for the sake of simplicity, this dataset only includes the primate families Atelidae, Cebidae, and two genera of Cercopithecidae (macaques and mandrills). The dataset also includes five numeric variables: mean species brain size, mean female brain size, mean male body mass, mean female body mass, and mass dimorphism. Furthermore, we took out any species within these families that did not have values for any of the variables. Let's load this data in first:

```{r}
f <- curl("https://raw.githubusercontent.com/laurabw/AN597_GroupProject_LLMC/master/kamcoop.simplified.2.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = TRUE)
head(d)
summary(d)
str(d) # There are 3 different families, this is probably what we will use for the analysis
plot(d$Family, d$Body_mass_male_mean) #plot mass and male body mass so we can start to see what differences we may be looking for 
```
This simple boxplot just shows what we will be looking for in the LDA; in terms of male body mass, Cercopithecidae and Atelidae are close to one another, but species within Cebidae are a bit smaller. 

Now we're going to look for this difference in body mass (male and female) using an LDA. We will do this by simply running the lda() wrapper function from the MASS package. 
```{r}
kamcoop.lda <- lda(Family ~ Body_mass_male_mean + Body_mass_female_mean, data = d) #categorical dependent variable is family 
kamcoop.lda
```
Output gives: group means, LD coefficients (lines that actually discriminate between the variables), prior proabilities of groups (what proportion of the data seemed to be in each group prior to starting), the proportion of trace is the percentage of separation achieved by each discriminant function (91% and 8%).

Now we can generate prediction values based on the LDA function and store it in an object by passing the model through the predict() function. The length of the value predicted will be correspond with the length of the processed data.

```{r}
lda.predict <- predict(kamcoop.lda)
lda.predict
```
 
These prediction values can also help us look at the accuracy of the LDA by creating a table between the predicted classes (families based off predicted values) and the actual families from the data.  
 
```{r}
lda.predict.class <- predict(kamcoop.lda,
                       newdata=d[,c(7,8)]
                       )$class #find prediction values of class
lda.predict.class #gives the classifications that are in the predictions
#determine how well the model fits by comparing predictions to 
table(lda.predict.class, d[,2])
``` 
This is a cross-tabulation of the real families and the predicted families. This basically is saying how many "Atelidae" were predicted to be "Atelidae" and so on. Based off of our table, our's is okay but not great. Only 6/12 Atelidae were correctly classified and 6 Cercopithecidae were misclassified as Atelidae. 
 
 
##Visualize LDA 
 
One way we can visualize the output of the LDA is to plot the predicted linear discriminants for each family on stacked histograms.
 
```{r}
ldahist(lda.predict$x[,1], g = d$Family)
ldahist(lda.predict$x[,2], g = d$Family)
```
 
As we saw earlier, Cercopithecidae has the most variance and is a bit all over the place for the first discriminant. So we can expect that they won't group together that well in a scatter plot. In the second, they all go to the left, so they will all probably overlap a bit in the scatter plot. 

Making a scatter plot: 
```{r}
newdata <- data.frame(Family = d[,2], lda = lda.predict$x) #make a dataframe with the familiy names and predicts LDs
newdata
library(ggplot2)
ggplot(newdata) + geom_point(aes(lda.LD1, lda.LD2, colour = Family), size = 2.5)
```
They are kind of on top of each other, so I'm going to try to add another variable, mass dimorphism, which we can use the same dataset: 

```{r}
kamcoop.lda.2 <- lda(Family ~ Body_mass_male_mean + Body_mass_female_mean + Mass_Dimorphism, data = d) #categorical dependent variable is family 
lda.predict.2 <- predict(kamcoop.lda.2)
lda.predict.2
newdata.2 <- data.frame(Family = d[,2], lda = lda.predict.2$x) 
newdata.2 #3 LD because 3 variables?
library(ggplot2)
ggplot(newdata.2) + geom_point(aes(lda.LD1, lda.LD2, colour = Family), size = 2.5) 
```
There is a little bit of differentiation between Cebidae and the other two families. This makes sense because Cebidae includes capuchins and squirrel monkeys, which are pretty small in comparison to atelids and cercopithecids.

Brain size???

```{r}
kamcoop.lda.3 <- lda(Family ~ Brain_Size_Species_Mean, data = d) 
lda.predict.3 <- predict(kamcoop.lda.3)
lda.predict.3
newdata.3 <- data.frame(Family = d[,2], lda = lda.predict.2$x) 
newdata.3 
ggplot(newdata.3) + geom_point(aes(lda.LD1, lda.LD2, colour = Family), size = 2.5) 
```

So with all of these variables, we are able to say that body mass and brain size can describe Cebidae out of these three families, but not the other two because they are too similar. 


##QDA##

(Insert info and pics in google drive)

Packages we need: 
```{r}
library(curl)
library(MASS)
library(ggplot2)
library(klaR)
```


##CHALLENGE##

Similar to what we did for LDA, we will now try to use QDA to try to determine which of our variables is the best predictor of which of the three families of primates individuals belong to. Our data was already loaded in earlier in the module but here it is in case you need it again.

```{r}
f <- curl("https://raw.githubusercontent.com/laurabw/AN597_GroupProject_LLMC/master/kamcoop.simplified.2.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = TRUE)
```

Now lets take a more expansive look at our data to see if our other variable differ between groups. 
```{r}
plot(d$Family, d$Body_mass_female_mean, main = "Mean Female Body Mass", ylab = "Body Mass (g)")
plot(d$Family, d$Brain_Size_Species_Mean, main = "Mean Species Brain Size", ylab = "Brain Size (g)")
plot(d$Family, d$Brain_Size_Female_Mean, main = "Mean Female Brain Size", ylab = "Brain Size (g)")
plot(d$Family, d$Mass_Dimorphism, main = "Sexual Dimorphism in Mass", ylab = "Sexual Dimorphism")
```
```{r}
scatterplotMatrix(~ Body_mass_male_mean + Body_mass_female_mean + Brain_Size_Species_Mean + Brain_Size_Female_Mean + Mass_Dimorphism| Family, data=d,legend = TRUE,smooth = list(method=gamLine),diagonal = TRUE,plot.points = TRUE)
?scatterplotMatrix
```

Most of our variables appear to be noticebly different between families but lets investigate further with QDA to see if one is a better predictor than the others. Instead of using the lda(), here we need to use the qda() wrapper which is also a function of the MASS package. We can start with model that includes all of our variables and we can simplify later if need be.

```{r}
kamcoop.qda <- qda(Family ~ Body_mass_male_mean + Body_mass_female_mean + Brain_Size_Species_Mean + Brain_Size_Female_Mean + Mass_Dimorphism, data = d) #categorical dependent variable is family 
kamcoop.qda
```

#QDA Partition Plots

```{r}
partimat(Family ~ Body_mass_male_mean + Body_mass_female_mean + Brain_Size_Species_Mean + Brain_Size_Female_Mean + Mass_Dimorphism, data = x, method="qda")
```

#QDA Predictions
```{r}
qda.predict <- predict(kamcoop.qda)
qda.predict
```


```{r}
qda.predict.class <- predict(kamcoop.qda,
                       newdata=d[,c(5,6,7,8,9)]
                       )$class #find prediction values of class
qda.predict.class #gives the classifications that are in the predictions
#determine how well the model fits by comparing predictions to 
table(qda.predict.class, d[,2])
``` 

Our model correctly predicted the family 100% of the time. This appaers to be more accurate than the LDA model using mean male and female body mass only. However, we should also test simpler models to see if we get similar results. As we learned this semester simpler is better when it comes to modeling.

```{r}
kamcoop.qda1 <- qda(Family ~ Body_mass_male_mean + Body_mass_female_mean + Brain_Size_Species_Mean + Brain_Size_Female_Mean, data = d) 
qda.predict.class1 <- predict(kamcoop.qda1,
                       newdata=d[,c(5,6,7,8)]
                       )$class #find prediction values of class
qda.predict.class1 #gives the classifications that are in the predictions
#determine how well the model fits by comparing predictions to 
table(qda.predict.class1, d[,2])
``` 

```{r}
kamcoop.qda2 <- qda(Family ~ Body_mass_male_mean + Body_mass_female_mean + Brain_Size_Species_Mean, data = d) 
qda.predict.class2 <- predict(kamcoop.qda2,
                       newdata=d[,c(5,7,8)]
                       )$class #find prediction values of class
qda.predict.class2 #gives the classifications that are in the predictions
#determine how well the model fits by comparing predictions to 
table(qda.predict.class2, d[,2])
``` 

```{r}
kamcoop.qda3 <- qda(Family ~ Body_mass_male_mean + Body_mass_female_mean, data = d) 
qda.predict.class3 <- predict(kamcoop.qda3,
                       newdata=d[,c(7,8)]
                       )$class #find prediction values of class
qda.predict.class3 #gives the classifications that are in the predictions
#determine how well the model fits by comparing predictions to 
table(qda.predict.class3, d[,2])
```

```{r}
kamcoop.qda4 <- qda(Family ~ Brain_Size_Species_Mean + Brain_Size_Female_Mean, data = d) 
qda.predict.class4 <- predict(kamcoop.qda4,
                       newdata=d[,c(5,6)]
                       )$class #find prediction values of class
qda.predict.class4 #gives the classifications that are in the predictions
#determine how well the model fits by comparing predictions to 
table(qda.predict.class4, d[,2])
```

It looks as though our best and simplest model is:

Family ~ Body_mass_male_mean + Body_mass_female_mean + Brain_Size_Species_Mean

<img src = "https://media.giphy.com/media/Q4r9RDlu5Si0o/giphy.gif">