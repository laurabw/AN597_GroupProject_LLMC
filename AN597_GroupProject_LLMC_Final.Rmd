---
title: "Discriminant Analysis"
author: "Laura Angley, Laura Brubaker-Wittman, Christian Gagnon, Mel Zarate"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    math: mathjax

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Preliminaries

For this module, you will need to install the following packages:

* {curl}
* {MASS}
* {candisc}
* {ggplot2}
* {klaR}
* {mvnormtest}

# Objectives

The objective of this module is to introduce **discriminant analysis** in general as well as describe and go through examples using three different types of disriminant analysis: **linear discriminant analysis (LDA)**, **quadratic discriminant analysis (QDA)**, and **canonical discriminant analysis (CDA)**.

# Introduction

### What is Discriminant Analysis?
**Discriminant analysis** is used when the objective of the assessment of data is to determine the adequacy of classification of data into groups, including a priori groups.This is different than a similar statistical tool, **principal component analysis (PCA)**, which focuses on overall variation in data without regard for a specific grouping of the observations.

In this way, **discriminant analysis** allows us to build classifiers, that is through this type of analysis, we can build a predictive model of group membership that we can then apply to new cases, to see into which group a new case would fall. As opposed to regression techniques, **discriminant analysis** produces group labels, and not a real value as its output.

Furthermore, **discriminant analysis** can be linear or it can fit other curves; it can also be two- or multi-dimensional with either a line or plane separting the categories, depending on the numnber of dimensions. We will see some of the examples of how this works and the kinds of visuals that can be produced as part of this tutorial.

While **discriminant analysis** parallels multiple regression analysis in some ways, one important difference is that regression analysis works with a continuous dependent variable, while **discriminant analysis** must use a discrete dependent variable.

A simple example of **discriminant analysis** would be to consider the following situation: An academic advisor splits a cohort of students into three groups: *those who graduate and do not plan to go on to graduate school*, *those who graduate and go on to graduate school*, and those who *drop out of college*.  A variety of data could be collected for each group during their time in college, before knowing which group they will fall into at the end. After graduation for that cohort, all the students will fall into one of these categories. **Discriminant analysis** could then be used to determine which variable(s) that were collected work as the best predictors of the students' educational outcomes.

## Linear Discriminant Analysis (LDA)

Just like a **PCA** or other **discriminant analysis**, *the goal is to see if certain qualities can describe a categorical variable*. If so, when we plot these analyses, we should be able to see somewhat clear separation of the categories. **LDA** uses data to divide predictor variables into categorical regions with linear boundaries. Compared to **PCA**, **LDA** orders dimensions based on how much separation each category achieves, maximizing the difference and minimizing the overlap of clusters. 

A **PCA** can be thought of as an “unsupervised” algorithm that ignores class labels and searches for the directions (principal components) that maximize variance. **LDA**, however, can be thought of as “supervised,” computing the directions (linear discriminants) that represent the axes in order to maximize separation between different classes. **LDA** tends to be better to use when there are multiple classes and the number of samples per class is relatively small. Also, in comparison with **QDA**, it is assumed that covariances of independent variables is the same for each class. 

<img src = "https://sebastianraschka.com/images/blog/2014/linear-discriminant-analysis/lda_1.png">

Here we have the blue AND green cluster on the x-axis, so if points in the future behave accordingly to the probability density function, then they should be classified into a single cluster.

The LDA returns and output giving scores that define the weight to how much the variables compose the function. These scores are calculated with this equation: 


\[ \delta_i (X) = - \frac{1}{2} \mu_i^T \Sigma^{-1}  X + ln(\pi_i)\]
