---
title: "Discriminant Analysis"
author: "Laura Angley, Laura Brubaker-Wittman, Christian Gagnon, Mel Zarate"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    math: mathjax

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Preliminaries

For this module, you will need to install the following packages:

* {curl}
* {MASS}
* {candisc}
* {ggplot2}
* {klaR}
* {mvnormtest}

# Objectives

The objective of this module is to introduce **discriminant analysis** in general as well as describe and go through examples using three different types of disriminant analysis: **linear discriminant analysis (LDA)**, **quadratic discriminant analysis (QDA)**, and **canonical discriminant analysis (CDA)**.

# Introduction

### What is Discriminant Analysis?
**Discriminant analysis** is used when the objective of the assessment of data is to determine the adequacy of classification of data into groups, including a priori groups.This is different than a similar statistical tool, **principal component analysis (PCA)**, which focuses on overall variation in data without regard for a specific grouping of the observations.

In this way, **discriminant analysis** allows us to build classifiers, that is through this type of analysis, we can build a predictive model of group membership that we can then apply to new cases, to see into which group a new case would fall. As opposed to regression techniques, **discriminant analysis** produces group labels, and not a real value as its output.

Furthermore, **discriminant analysis** can be linear or it can fit other curves; it can also be two- or multi-dimensional with either a line or plane separting the categories, depending on the numnber of dimensions. We will see some of the examples of how this works and the kinds of visuals that can be produced as part of this tutorial.

While **discriminant analysis** parallels multiple regression analysis in some ways, one important difference is that regression analysis works with a continuous dependent variable, while **discriminant analysis** must use a discrete dependent variable.

A simple example of **discriminant analysis** would be to consider the following situation: An academic advisor splits a cohort of students into three groups: *those who graduate and do not plan to go on to graduate school*, *those who graduate and go on to graduate school*, and those who *drop out of college*.  A variety of data could be collected for each group during their time in college, before knowing which group they will fall into at the end. After graduation for that cohort, all the students will fall into one of these categories. **Discriminant analysis** could then be used to determine which variable(s) that were collected work as the best predictors of the students' educational outcomes.

## Linear Discriminant Analysis (LDA)

Just like a **PCA** or other **discriminant analysis**, *the goal is to see if certain qualities can describe a categorical variable*. If so, when we plot these analyses, we should be able to see somewhat clear separation of the categories. **LDA** uses data to divide predictor variables into categorical regions with linear boundaries. Compared to **PCA**, **LDA** orders dimensions based on how much separation each category achieves, maximizing the difference and minimizing the overlap of clusters. 

A **PCA** can be thought of as an “unsupervised” algorithm that ignores class labels and searches for the directions (principal components) that maximize variance. **LDA**, however, can be thought of as “supervised,” computing the directions (linear discriminants) that represent the axes in order to maximize separation between different classes. **LDA** tends to be better to use when there are multiple classes and the number of samples per class is relatively small. Also, in comparison with **QDA**, it is assumed that covariances of independent variables is the same for each class. 

<img src = "https://sebastianraschka.com/images/blog/2014/linear-discriminant-analysis/lda_1.png">

Here we have the blue AND green cluster on the x-axis, so if points in the future behave accordingly to the probability density function, then they should be classified into a single cluster.

The LDA returns and output giving scores that define the weight to how much the variables compose the function. These scores are calculated with this equation: 


\[ \delta_i (X) = - \frac{1}{2} \mu_i^T \Sigma^{-1}  X + ln(\pi_i)\]

* **δ** is the discriminant score for class **i**
* **X** is the matrix of independent variables
* **μ** is a vector containing the means of each variable for class **i**
* **Σ** is the covariance matrix of the variables (assumed to be the same for all classes)
* **π** is the prior probability that an observation belongs to class **i**

But why would we ever do this by hand when we have the lovely lda() function in the {MASS} ***R*** package?!

<img src = "https://www.lostandfoundnature.com/wp-content/uploads/Blog/Spreadwing_damselfly/Lagothrix-flavicauda-yellow-tailed-woolly-monkey-Andrew-WalmsleyNPC-3-1024x680.jpg">

We can use the Kamilar and Cooper data set to take a look at how this works. Let's load this data in first:

```{r}
library(curl)
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN597_Fall19/KamilarAndCooperData.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = TRUE)
head(d)
summary(d)
plot(d[,c(8,9)],
     col=d[,2]) #plot female and male brain size, with different colors being different families (families seem to group together a bit better, and there arent as many as the number of genera) 
```

Now we can try the lda() function:

```{r}
library(MASS)
kamcoop.lda <- lda(Family ~ Body_mass_male_mean + Body_mass_female_mean, data = d) #categorical dependent variable is family 
kamcoop.lda
```

Output gives: group means, LD coefficients (lines that actually discriminate between the variables), prior probabilities of groups (what proportion of the data seemed to be in each group prior to starting).

The predict() function generates a value from a selected model function. The length of the value predicted will correspond with the length of the processed data. 

NEED TO ADD REST OF LDA ONCE FIX CODE AND ADD QDA ONCE TEXT AND CODE IS READY TO BE ADDED


<img src = "https://media.giphy.com/media/Q4r9RDlu5Si0o/giphy.gif">