---
title: "Discriminant Analysis"
author: "Laura Angley, Laura Brubaker-Wittman, Christian Gagnon, Mel Zarate"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    math: mathjax

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Preliminaries

For this module, you will need to install the following packages:

* {curl}
* {MASS}
* {candisc}
* {ggplot2}
* {klaR}
* {mvnormtest}
* {car}

# Objectives

The objective of this module is to introduce **discriminant analysis** in general as well as describe and go through examples using three different types of disriminant analysis: **linear discriminant analysis (LDA)**, **quadratic discriminant analysis (QDA)**, and **canonical discriminant analysis (CDA)**.

# Introduction

### What is Discriminant Analysis?
**Discriminant analysis** is used when the objective of the assessment of data is to determine the adequacy of classification of data into groups, including a priori groups.This is different than a similar statistical tool, **principal component analysis (PCA)**, which focuses on overall variation in data without regard for a specific grouping of the observations.

In this way, **discriminant analysis** allows us to build classifiers, that is through this type of analysis, we can build a predictive model of group membership that we can then apply to new cases, to see into which group a new case would fall. As opposed to regression techniques, **discriminant analysis** produces group labels, and not a real value as its output.

Furthermore, **discriminant analysis** can be linear or it can fit other curves; it can also be two- or multi-dimensional with either a line or plane separting the categories, depending on the numnber of dimensions. We will see some of the examples of how this works and the kinds of visuals that can be produced as part of this tutorial.

While **discriminant analysis** parallels multiple regression analysis in some ways, one important difference is that regression analysis works with a continuous dependent variable, while **discriminant analysis** must use a discrete dependent variable.

A simple example of **discriminant analysis** would be to consider the following situation: An academic advisor splits a cohort of students into three groups: *those who graduate and do not plan to go on to graduate school*, *those who graduate and go on to graduate school*, and those who *drop out of college*.  A variety of data could be collected for each group during their time in college, before knowing which group they will fall into at the end. After graduation for that cohort, all the students will fall into one of these categories. **Discriminant analysis** could then be used to determine which variable(s) that were collected work as the best predictors of the students' educational outcomes.

## Linear Discriminant Analysis (LDA)

Just like a **PCA** or other **discriminant analysis**, *the goal is to see if certain qualities can describe a categorical variable*. If so, when we plot these analyses, we should be able to see somewhat clear separation of the categories. **LDA** uses data to divide predictor variables into categorical regions with linear boundaries. Compared to **PCA**, **LDA** orders dimensions based on how much separation each category achieves, maximizing the difference and minimizing the overlap of clusters. 

A **PCA** can be thought of as an “unsupervised” algorithm that ignores class labels and searches for the directions (principal components) that maximize variance. **LDA**, however, can be thought of as “supervised,” computing the directions (linear discriminants) that represent the axes in order to maximize separation between different classes. **LDA** tends to be better to use when there are multiple classes and the number of samples per class is relatively small. Also, in comparison with **QDA**, it is assumed that covariances of independent variables is the same for each class. 

<img src = "https://sebastianraschka.com/images/blog/2014/linear-discriminant-analysis/lda_1.png">

Here we have the blue AND green cluster on the x-axis, so if points in the future behave accordingly to the probability density function, then they should be classified into a single cluster.

The LDA returns and output giving scores that define the weight to how much the variables compose the function. These scores are calculated with this equation: 


\[ \delta_i (X) = - \frac{1}{2} \mu_i^T \Sigma^{-1}  X + ln(\pi_i)\]

* **δ** is the discriminant score for class **i**
* **X** is the matrix of independent variables
* **μ** is a vector containing the means of each variable for class **i**
* **Σ** is the covariance matrix of the variables (assumed to be the same for all classes)
* **π** is the prior probability that an observation belongs to class **i**

But why would we ever do this by hand when we have the lovely lda() function in the {MASS} ***R*** package?!

<img src = "https://www.lostandfoundnature.com/wp-content/uploads/Blog/Spreadwing_damselfly/Lagothrix-flavicauda-yellow-tailed-woolly-monkey-Andrew-WalmsleyNPC-3-1024x680.jpg">

### LDA CHALLENGE

Packages we need: 
```{r}
library(curl)
library(MASS)
library(ggplot2)
```

We can use the Kamilar and Cooper data set to take a look at how this works. We're going to use the Kamala and Cooper dataset, but, for the sake of simplicity, this dataset only includes the primate families Atelidae, Cebidae, and two genera of Cercopithecidae (macaques and mandrills). The dataset also includes five numeric variables: mean species brain size, mean female brain size, mean male body mass, mean female body mass, and mass dimorphism. Furthermore, we took out any species within these families that did not have values for any of the variables. Let's load this data in first:

```{r}
f <- curl("https://raw.githubusercontent.com/laurabw/AN597_GroupProject_LLMC/master/kamcoop.simplified.2.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = TRUE)
head(d)
summary(d)
str(d) # There are 3 different families, this is probably what we will use for the analysis
plot(d$Family, d$Body_mass_male_mean) #plot mass and male body mass so we can start to see what differences we may be looking for 
```
This simple boxplot just shows what we will be looking for in the LDA; in terms of male body mass, Cercopithecidae and Atelidae are close to one another, but species within Cebidae are a bit smaller. 

Now we're going to look for this difference in body mass (male and female) using an LDA. We will do this by simply running the lda() wrapper function from the MASS package. 
```{r}
kamcoop.lda <- lda(Family ~ Body_mass_male_mean + Body_mass_female_mean, data = d) #categorical dependent variable is family 
kamcoop.lda
```
Output gives: group means, LD coefficients (lines that actually discriminate between the variables), prior proabilities of groups (what proportion of the data seemed to be in each group prior to starting), the proportion of trace is the percentage of separation achieved by each discriminant function (91% and 8%).

Now we can generate prediction values based on the LDA function and store it in an object by passing the model through the predict() function. The length of the value predicted will be correspond with the length of the processed data.

```{r}
lda.predict <- predict(kamcoop.lda)
lda.predict
```
 
These prediction values can also help us look at the accuracy of the LDA by creating a table between the predicted classes (families based off predicted values) and the actual families from the data.  
 
```{r}
lda.predict.class <- predict(kamcoop.lda,
                       newdata=d[,c(7,8)]
                       )$class #find prediction values of class
lda.predict.class #gives the classifications that are in the predictions
#determine how well the model fits by comparing predictions to 
table(lda.predict.class, d[,2])
``` 
This is a cross-tabulation of the real families and the predicted families. This basically is saying how many "Atelidae" were predicted to be "Atelidae" and so on. Based off of our table, our's is okay but not great. Only 6/12 Atelidae were correctly classified and 6 Cercopithecidae were misclassified as Atelidae. 
 
 
### Visualize LDA 
 
One way we can visualize the output of the LDA is to plot the predicted linear discriminants for each family on stacked histograms.
 
```{r}
ldahist(lda.predict$x[,1], g = d$Family)
ldahist(lda.predict$x[,2], g = d$Family)
```
 
As we saw earlier, Cercopithecidae has the most variance and is a bit all over the place for the first discriminant. So we can expect that they won't group together that well in a scatter plot. In the second, they all go to the left, so they will all probably overlap a bit in the scatter plot. 

Making a scatter plot: 
```{r}
newdata <- data.frame(Family = d[,2], lda = lda.predict$x) #make a dataframe with the familiy names and predicts LDs
newdata
library(ggplot2)
ggplot(newdata) + geom_point(aes(lda.LD1, lda.LD2, colour = Family), size = 2.5)
```
They are kind of on top of each other, so I'm going to try to add another variable, mass dimorphism, which we can use the same dataset: 

```{r}
kamcoop.lda.2 <- lda(Family ~ Body_mass_male_mean + Body_mass_female_mean + Mass_Dimorphism, data = d) #categorical dependent variable is family 
lda.predict.2 <- predict(kamcoop.lda.2)
lda.predict.2
newdata.2 <- data.frame(Family = d[,2], lda = lda.predict.2$x) 
newdata.2 #3 LD because 3 variables?
library(ggplot2)
ggplot(newdata.2) + geom_point(aes(lda.LD1, lda.LD2, colour = Family), size = 2.5) 
```
There is a little bit of differentiation between Cebidae and the other two families. This makes sense because Cebidae includes capuchins and squirrel monkeys, which are pretty small in comparison to atelids and cercopithecids.

Brain size???

```{r}
kamcoop.lda.3 <- lda(Family ~ Brain_Size_Species_Mean, data = d) 
lda.predict.3 <- predict(kamcoop.lda.3)
lda.predict.3
newdata.3 <- data.frame(Family = d[,2], lda = lda.predict.2$x) 
newdata.3 
ggplot(newdata.3) + geom_point(aes(lda.LD1, lda.LD2, colour = Family), size = 2.5) 
```

So with all of these variables, we are able to say that body mass and brain size can describe Cebidae out of these three families, but not the other two because they are too similar. 

# Quadratic Discriminant Analysis (QDA)

Just like ANOVA, Linear Discriminant Analyses(LDA) assumes equal variance between our input variables. If the data fails to meet this criteria, observation are more likely to be assigned to the class showing the greatest variability. This type of bias error is the reason the Quadratic Discriminant Analysis (QDA) was invented. In other words, *LDA* and *QDA* are both classification technique but, *QDA* is less conservative by allowing different variance matrices for different classes. Another key difference between *LDA* and *QDA* is that the former assigns a linear classification decision boundary while the later leads to a quadratic decision boundary.

<img src = "https://scikit-learn.org/dev/_images/sphx_glr_plot_lda_qda_0011.png">


* Top row: when the covariance matrices are indeed the same in the data, LDA and QDA lead to the same decision boundaries.
* Bottom row: when the covariance matrices are different, LDA leads to bad performance as its assumption becomes invalid, while QDA performs classification much better.

Both *LDA* and *QDA* can be derived from the same probabilistic models that models the class conditional distribution of the data $P(X|y=k)$ for each class $k$. *QDA* provides an alternative approach by assuming that each class has its own covariance matrix $\Sigma_k$.
In order to calculate the quadratic boundary we use the following equation:

\[\delta_k(x) = \log \pi_k - \frac{1}{2} \log |\Sigma_k| - \frac{1}{2} (x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k)\]

* **δ** is the discriminant score for class **k**
* **X** is the matrix of independent variables
* **μ** is a vector containing the means of each variable for class **k**
* **Σ** is the covariance matrix of the variables (assumed to be the same for all classes)
* **π** is the prior probability that an observation belongs to class **k**

Let's do a little exercise to see how *QDA* can be applied to our simplified Kamilar and Cooper dataset and how it differs from *LDA*.

Packages we need: 
```{r}
library(curl)
library(MASS)
library(ggplot2)
library(klaR)
library(car)
```


### QDA CHALLENGE

Similar to what we did for LDA, we will now try to use QDA to try to determine which variable or combination of variables will give us the best predictor model for which of the three families of primates an individual belongs. Our data was already loaded in earlier in the module but here it is in case you need it again.

```{r}
f <- curl("https://raw.githubusercontent.com/laurabw/AN597_GroupProject_LLMC/master/kamcoop.simplified.2.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = TRUE)
```

Now lets take a more expansive look at our data to see if our other variables differ between groups. 

```{r}
plot(d$Family, d$Body_mass_female_mean, main = "Mean Female Body Mass", ylab = "Body Mass (g)")
plot(d$Family, d$Brain_Size_Species_Mean, main = "Mean Species Brain Size", ylab = "Brain Size (g)")
plot(d$Family, d$Brain_Size_Female_Mean, main = "Mean Female Brain Size", ylab = "Brain Size (g)")
plot(d$Family, d$Mass_Dimorphism, main = "Sexual Dimorphism in Mass", ylab = "Sexual Dimorphism")
```

Another way we can visualize the variation in our data is by using the scatterplotMatrix() wrapper function from the *car* package.

```{r}
scatterplotMatrix(~ Body_mass_male_mean + Body_mass_female_mean + Brain_Size_Species_Mean + Brain_Size_Female_Mean + Mass_Dimorphism| Family, data=d,legend = TRUE,smooth = list(method=gamLine),diagonal = TRUE,plot.points = TRUE)
```

Most of our variables appear to be noticebly different between families but lets investigate further with QDA to see if one is a better predictor than the others. Instead of using the lda(), here we need to use the qda() wrapper which is also a function of the MASS package. We can start with model that includes all of our variables and we can simplify later if need be.

```{r}
kamcoop.qda <- qda(Family ~ Body_mass_male_mean + Body_mass_female_mean + Brain_Size_Species_Mean + Brain_Size_Female_Mean + Mass_Dimorphism, data = d) #categorical dependent variable is family 
kamcoop.qda
```

### QDA Partition Plots

Next we can use the partimat() function from the *klaR* package which allows us to plot the results of the *QDA*. This will create a figure for every possible combination of two variables from our data. 

```{r}
partimat(Family ~ Body_mass_male_mean + Body_mass_female_mean + Brain_Size_Species_Mean + Brain_Size_Female_Mean + Mass_Dimorphism, data = d, method="qda")
```

Think of each of these plots as a different prespective on our data. The different colored regions outline each classification area. Observation that falls within a region is predicted to be from a specific class. The apparent error rate at the top of each figure imforms us about how many observations fall outside their predicted range.

### QDA Predictions

The predict function works identically to the way we used it for *LDA*. We will again use this function to create a confusion matrix for our data.

```{r}
qda.predict <- predict(kamcoop.qda)
qda.predict
```

This doesn't tell us much. Let's create a matrix like we did with *LDA*.

```{r}
qda.predict.class <- predict(kamcoop.qda,
                       newdata=d[,c(5,6,7,8,9)]
                       )$class
qda.predict.class
table(qda.predict.class, d[,2])
``` 

Our model correctly predicted the correct family 100% of the time. This appears to be more accurate than the LDA model using mean male and female body mass only. However, we should also test simpler models to see if we get similar results. As we learned this semester simpler is better when it comes to modeling.

Let's test some other modeling options!

Removing Mass_Dimorphism:
```{r}
kamcoop.qda1 <- qda(Family ~ Body_mass_male_mean + Body_mass_female_mean + Brain_Size_Species_Mean + Brain_Size_Female_Mean, data = d) 
qda.predict.class1 <- predict(kamcoop.qda1,
                       newdata=d[,c(5,6,7,8)]
                       )$class
qda.predict.class1
table(qda.predict.class1, d[,2])
``` 
Still looking good!

Now let's try without Mass_Dimorphism and Brain_Size_Female_Mean:
```{r}
kamcoop.qda2 <- qda(Family ~ Body_mass_male_mean + Body_mass_female_mean + Brain_Size_Species_Mean, data = d) 
qda.predict.class2 <- predict(kamcoop.qda2,
                       newdata=d[,c(5,7,8)]
                       )$class
qda.predict.class2
table(qda.predict.class2, d[,2])
``` 
Still 100%

Now we try with just Male and Female body mass:
```{r}
kamcoop.qda3 <- qda(Family ~ Body_mass_male_mean + Body_mass_female_mean, data = d) 
qda.predict.class3 <- predict(kamcoop.qda3,
                       newdata=d[,c(7,8)]
                       )$class
qda.predict.class3
table(qda.predict.class3, d[,2])
```
And now we see we are starting to lose accuracy.

One more combo just for the sake of being thorough without body mass or sexual dimorphism:
```{r}
kamcoop.qda4 <- qda(Family ~ Brain_Size_Species_Mean + Brain_Size_Female_Mean, data = d) 
qda.predict.class4 <- predict(kamcoop.qda4,
                       newdata=d[,c(5,6)]
                       )$class
qda.predict.class4
table(qda.predict.class4, d[,2])
```
Not very good!

It looks as though our best and simplest model is:

Family ~ Body_mass_male_mean + Body_mass_female_mean + Brain_Size_Species_Mean

As such, we can use this model and QDA to predict how an unknown sample should be classified into one of our three Families of primates.

# Canonical Discriminant Analysis (CDA)

Canonical discriminant analysis (CDA) is a multivariate statistical technique that identifies differences among groups of treatments (the independent variables) and shows us the relationships between the various (dependent) variables within those groups. CDA determines the best way to discriminate the treatment groups from each other using quantitative measurements of all the variables within each group.

Running the CDA yields several **uncorrelated** canonical discriminant functions (CDFs). These linear combinations of the variables best separate the original treatment group means relative to within group variation. A lack of correlation between all of the CDFs allows each one to pull out a completely new dimension of information from the groups and variables. CDF1 shows the maximum possible variations among groups, therefore yielding the greatest degree of group differences. CDF1 and CDF2 are completely uncorrelated so CDF2 will reflect differences not shown in CDF1. Similarly, CDF1 and CDF2 are uncorrelated to CDF3, which reflects its own unique differences between groups.

Time to load in some data! To all my true crime lovers out there...

<img src = "https://media.giphy.com/media/9VnK7N0jOB1zBGFrfE/giphy.gif">

Load in the USA Arrests dataset, which shows number of arrests per 100,000 residents for assault, murder, and burglary in each of the 50 US states in 1973 as well as the percentage of residents living in urban areas in each state. (This dataset has been modified)

<img src = "https://media.giphy.com/media/Q4r9RDlu5Si0o/giphy.gif">